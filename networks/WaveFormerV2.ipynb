{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "012e75ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from segformer import *\n",
    "from typing import Tuple\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import pywt\n",
    "import math\n",
    "\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.vision_transformer import _cfg\n",
    "\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable, gradcheck\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b145a",
   "metadata": {},
   "source": [
    "The input size is \n",
    "data shape--------- torch.Size([1, 1, 224, 224]) torch.Size([1, 1, 224, 224])\n",
    "\n",
    "image: torch.Size([1, 1, 224, 224])\n",
    "\n",
    "label: torch.Size([1, 1, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753ab970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the input of size torch.Size([1, 1, 224, 224])\n",
      "Test the input of size torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.rand(1, 1, 224, 224)\n",
    "print(\"Test the input of size {}\".format(inputs.shape))\n",
    "if inputs.size()[1] == 1:\n",
    "    inputs = inputs.repeat(1,3,1,1)\n",
    "print(\"Test the input of size {}\".format(inputs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04709c64",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b03a15b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74432db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pywt\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable, gradcheck\n",
    "\n",
    "class DWT_Function(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, w_ll, w_lh, w_hl, w_hh):\n",
    "        x = x.contiguous()\n",
    "        ctx.save_for_backward(w_ll, w_lh, w_hl, w_hh)\n",
    "        ctx.shape = x.shape\n",
    "\n",
    "        dim = x.shape[1]\n",
    "        x_ll = torch.nn.functional.conv2d(x, w_ll.expand(dim, -1, -1, -1), stride = 2, groups = dim)\n",
    "        x_lh = torch.nn.functional.conv2d(x, w_lh.expand(dim, -1, -1, -1), stride = 2, groups = dim)\n",
    "        x_hl = torch.nn.functional.conv2d(x, w_hl.expand(dim, -1, -1, -1), stride = 2, groups = dim)\n",
    "        x_hh = torch.nn.functional.conv2d(x, w_hh.expand(dim, -1, -1, -1), stride = 2, groups = dim)\n",
    "        x = torch.cat([x_ll, x_lh, x_hl, x_hh], dim=1)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dx):\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            w_ll, w_lh, w_hl, w_hh = ctx.saved_tensors\n",
    "            B, C, H, W = ctx.shape\n",
    "            dx = dx.view(B, 4, -1, H//2, W//2)\n",
    "\n",
    "            dx = dx.transpose(1,2).reshape(B, -1, H//2, W//2)\n",
    "            filters = torch.cat([w_ll, w_lh, w_hl, w_hh], dim=0).repeat(C, 1, 1, 1)\n",
    "            dx = torch.nn.functional.conv_transpose2d(dx, filters, stride=2, groups=C)\n",
    "\n",
    "        return dx, None, None, None, None\n",
    "\n",
    "    \n",
    "class DWT_2D(nn.Module):\n",
    "    def __init__(self, wave):\n",
    "        super(DWT_2D, self).__init__()\n",
    "        w = pywt.Wavelet(wave)\n",
    "        dec_hi = torch.Tensor(w.dec_hi[::-1]) \n",
    "        dec_lo = torch.Tensor(w.dec_lo[::-1])\n",
    "\n",
    "        w_ll = dec_lo.unsqueeze(0)*dec_lo.unsqueeze(1)\n",
    "        w_lh = dec_lo.unsqueeze(0)*dec_hi.unsqueeze(1)\n",
    "        w_hl = dec_hi.unsqueeze(0)*dec_lo.unsqueeze(1)\n",
    "        w_hh = dec_hi.unsqueeze(0)*dec_hi.unsqueeze(1)\n",
    "\n",
    "        self.register_buffer('w_ll', w_ll.unsqueeze(0).unsqueeze(0))\n",
    "        self.register_buffer('w_lh', w_lh.unsqueeze(0).unsqueeze(0))\n",
    "        self.register_buffer('w_hl', w_hl.unsqueeze(0).unsqueeze(0))\n",
    "        self.register_buffer('w_hh', w_hh.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "        self.w_ll = self.w_ll.to(dtype=torch.float32)\n",
    "        self.w_lh = self.w_lh.to(dtype=torch.float32)\n",
    "        self.w_hl = self.w_hl.to(dtype=torch.float32)\n",
    "        self.w_hh = self.w_hh.to(dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return DWT_Function.apply(x, self.w_ll, self.w_lh, self.w_hl, self.w_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb6e6e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        input  -> x:[B, D, H, W]\n",
    "        output ->   [B, D, H, W]\n",
    "    \n",
    "        in_channels:    int -> Embedding Dimension \n",
    "        key_channels:   int -> Key Embedding Dimension,   Best: (in_channels)\n",
    "        value_channels: int -> Value Embedding Dimension, Best: (in_channels or in_channels//2) \n",
    "        head_count:     int -> It divides the embedding dimension by the head_count and process each part individually\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, key_channels, value_channels, head_count=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.key_channels = key_channels\n",
    "        self.head_count = head_count\n",
    "        self.value_channels = value_channels\n",
    "\n",
    "        self.keys = nn.Conv2d(in_channels, key_channels, 1) \n",
    "        self.queries = nn.Conv2d(in_channels, key_channels, 1)\n",
    "        self.values = nn.Conv2d(in_channels, value_channels, 1)\n",
    "        self.reprojection = nn.Conv2d(value_channels, in_channels, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        n, _, h, w = input_.size()\n",
    "        \n",
    "        keys = self.keys(input_).reshape((n, self.key_channels, h * w))\n",
    "        queries = self.queries(input_).reshape(n, self.key_channels, h * w)\n",
    "        values = self.values(input_).reshape((n, self.value_channels, h * w))\n",
    "        \n",
    "        head_key_channels = self.key_channels // self.head_count\n",
    "        head_value_channels = self.value_channels // self.head_count\n",
    "        \n",
    "        attended_values = []\n",
    "        for i in range(self.head_count):\n",
    "            key = F.softmax(keys[\n",
    "                :,\n",
    "                i * head_key_channels: (i + 1) * head_key_channels,\n",
    "                :\n",
    "            ], dim=2)\n",
    "            \n",
    "            query = F.softmax(queries[\n",
    "                :,\n",
    "                i * head_key_channels: (i + 1) * head_key_channels,\n",
    "                :\n",
    "            ], dim=1)\n",
    "                        \n",
    "            value = values[\n",
    "                :,\n",
    "                i * head_value_channels: (i + 1) * head_value_channels,\n",
    "                :\n",
    "            ]            \n",
    "            \n",
    "            context = key @ value.transpose(1, 2) # dk*dv\n",
    "            attended_value = (context.transpose(1, 2) @ query).reshape(n, head_value_channels, h, w) # n*dv            \n",
    "            attended_values.append(attended_value)\n",
    "                \n",
    "        aggregated_values = torch.cat(attended_values, dim=1)\n",
    "        attention = self.reprojection(aggregated_values)\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4c017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bridge=False):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.bridge = False\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.kv = nn.Linear(dim, dim * 2)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, H, W, q=None):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        if self.bridge:\n",
    "            q = q.reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        \n",
    "        kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv[0], kv[1]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec6f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_gussian(kernel_size, Sigma=1, in_channels = 64):\n",
    "    \n",
    "    kernel_weights = cv2.getGaussianKernel(ksize=kernel_size, sigma= Sigma)\n",
    "    kernel_weights = kernel_weights * kernel_weights.T\n",
    "    kernel_weights = np.repeat(kernel_weights[None, ...], in_channels, axis=0)[:, None, ...]\n",
    "\n",
    "    return kernel_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00b778c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1, 3, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_kernel_gussian(3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21333f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FET(nn.Module):\n",
    "    def __init__(self, dim, num_heads, sr_ratio, bridge=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bridge = bridge\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "        self.sr_ratio = sr_ratio\n",
    "        \n",
    "        self.dwt = DWT_2D(wave='haar')\n",
    "        self.reduce = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim//4, kernel_size=1, padding=0, stride=1),\n",
    "            nn.BatchNorm2d(dim//4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.filter = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=1, stride=1, groups=1),\n",
    "            nn.BatchNorm2d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.kv_embed = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio) if sr_ratio > 1 else nn.Identity()\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.kv = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim * 2)\n",
    "        )\n",
    "        \n",
    "        self.hf_agg = nn.Conv3d(dim//4, dim//4, kernel_size=(3, 1, 1), bias=False, groups=dim//4)\n",
    "        \n",
    "        # Gaussian Kernel \n",
    "        ### parameters\n",
    "        kernet_shapes = [3, 5]\n",
    "        s_value = np.power(2, 1/3)\n",
    "        sigma   = 1.6\n",
    "\n",
    "        ### Kernel weights for Laplacian pyramid\n",
    "        self.sigma1_kernel = get_kernel_gussian(kernel_size = kernet_shapes[0], Sigma = sigma*np.power(s_value, 1),\n",
    "                                           in_channels = dim//4)\n",
    "        self.sigma1_kernel = torch.from_numpy(self.sigma1_kernel).float().to(device)\n",
    "        \n",
    "        self.sigma2_kernel = get_kernel_gussian(kernel_size = kernet_shapes[1], Sigma = sigma*np.power(s_value, 2),\n",
    "                                           in_channels = dim//4)    \n",
    "        self.sigma2_kernel = torch.from_numpy(self.sigma2_kernel).float().to(device)\n",
    "        \n",
    "        self.boundary_lvl_agg = nn.Conv3d(dim//4, dim//4, kernel_size=(3, 1, 1), bias=False, groups=dim//4)\n",
    "        \n",
    "        self.linear_upsample = nn.Linear(dim//4, dim)\n",
    "        \n",
    "        self.proj = nn.Linear(dim+dim, dim)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x, H, W, q=None):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        if self.bridge:\n",
    "            q = q.reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        else:\n",
    "            q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        x = x.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        x_dwt = self.dwt(self.reduce(x))\n",
    "        x_dwt_filter = self.filter(x_dwt)\n",
    "\n",
    "        kv = self.kv_embed(x_dwt_filter).reshape(B, C, -1).permute(0, 2, 1)\n",
    "        kv = self.kv(kv).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv[0], kv[1]\n",
    "        \n",
    "        # Spatial Attention \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        \n",
    "        # Efficient Attention\n",
    "        global_context = k.reshape(B, N//4, C).transpose(1, 2) @ v.reshape(B, N//4, C)\n",
    "        out_efficient_att = q.reshape(B, N, C) @ global_context\n",
    "        \n",
    "        \n",
    "        # Boundary Attention\n",
    "        x_dwt_hf = Rearrange('b (p c) h w -> b c p h w', p=4)(x_dwt)[:, :, 1:, ...]\n",
    "        x_hf_agg = self.hf_agg(x_dwt_hf)[:, :, 0, ...]\n",
    "        \n",
    "        G0 = x_hf_agg\n",
    "        G1 = F.conv2d(input=x_hf_agg, weight=self.sigma1_kernel, bias=None, padding='same', groups=self.dim//4)\n",
    "        G2 = F.conv2d(input=x_hf_agg, weight=self.sigma2_kernel, bias=None, padding='same', groups=self.dim//4)        \n",
    "        \n",
    "        L0 = G0[:,:, None, ...]                 # Level 1      \n",
    "        L1 = torch.sub(G0, G1)[:,:, None, ...]  # Level 2   \n",
    "        L2 = torch.sub(G1, G2)[:,:, None, ...]  # Level 3        \n",
    "        \n",
    "        lvl_cat = torch.cat([L0, L1, L2], dim= 2)\n",
    "        boundary_lvl_agg = self.boundary_lvl_agg(lvl_cat)[:, :, 0, ...].permute(0, 2, 3, 1)\n",
    "        boundary_att = self.linear_upsample(boundary_lvl_agg).permute(0, 3, 1, 2)\n",
    "        \n",
    "        \n",
    "        # Value and Boundary Attention Summation\n",
    "        boundary_att = Rearrange('b (n p) h w -> b n (h w) p', n=self.num_heads)(boundary_att)\n",
    "        v_sum_boundary_att = v + boundary_att\n",
    "        \n",
    "        \n",
    "        # Spatial Attention @ Enhanced Value \n",
    "        out_spatial_boundary = (attn @ v_sum_boundary_att).transpose(1, 2).reshape(B, N, C)\n",
    "        \n",
    "        \n",
    "        # Final Projection\n",
    "        out = self.proj(torch.cat([out_spatial_boundary, out_efficient_att], dim=-1))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adcebe47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 64 \n",
    "head = 16\n",
    "h_or_w = 56\n",
    "hw = h_or_w**2\n",
    "sr_ratio = 1\n",
    "\n",
    "x_test = FET(dim, head, sr_ratio).to(device)\n",
    "x_test(torch.rand(1, hw, dim).to(device), h_or_w, h_or_w).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4616c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FETBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Input  -> x (Size: (b, (H*W), d)), H, W\n",
    "        Output -> (b, (H*W), d)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, num_heads, sr_ratio, token_mlp='mix_skip', bottleneck=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(in_dim)\n",
    "        \n",
    "        if bottleneck:\n",
    "            self.attn = Attention(in_dim, num_heads)\n",
    "        else:\n",
    "            self.attn = FET(in_dim, num_heads, sr_ratio)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(in_dim)\n",
    "        if token_mlp=='mix':\n",
    "            self.mlp = MixFFN(in_dim, int(in_dim*4))  \n",
    "        elif token_mlp=='mix_skip':\n",
    "            self.mlp = MixFFN_skip(in_dim, int(in_dim*4)) \n",
    "        else:\n",
    "            self.mlp = MLP_FFN(in_dim, int(in_dim*4))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, H, W) -> torch.Tensor:\n",
    "        x = x + self.attn(self.norm1(x), H, W)        \n",
    "        x = x + self.mlp(self.norm2(x), H, W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d00c6484",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = FETBlock(in_dim=64, num_heads=2, sr_ratio=1, token_mlp='mix_skip').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a60c0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(torch.rand(1, 3136, 64).cuda(), 56, 56).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b7901c",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fef17b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, image_size, in_dim, num_heads, sr_ratio, layers, token_mlp='mix_skip'):\n",
    "        super().__init__()\n",
    "        patch_sizes = [7, 3, 3, 3]\n",
    "        strides = [4, 2, 2, 2]\n",
    "        padding_sizes = [3, 1, 1, 1]\n",
    "\n",
    "        \n",
    "        # patch_embed\n",
    "        # layers = [2, 2, 2, 2] dims = [64, 128, 320, 512]\n",
    "        self.patch_embed1 = OverlapPatchEmbeddings(image_size, patch_sizes[0], strides[0], padding_sizes[0], 3, in_dim[0])\n",
    "        self.patch_embed2 = OverlapPatchEmbeddings(image_size//4, patch_sizes[1], strides[1], padding_sizes[1],in_dim[0], in_dim[1])\n",
    "        self.patch_embed3 = OverlapPatchEmbeddings(image_size//8, patch_sizes[2], strides[2], padding_sizes[2],in_dim[1], in_dim[2])\n",
    "        self.patch_embed4 = OverlapPatchEmbeddings(image_size//16, patch_sizes[3], strides[3], padding_sizes[3],in_dim[2], in_dim[3])\n",
    "        \n",
    "        # transformer encoder\n",
    "        self.block1 = nn.ModuleList([ \n",
    "            FETBlock(in_dim[0], num_heads[0], sr_ratio[0], token_mlp)\n",
    "        for _ in range(layers[0])])\n",
    "        self.norm1 = nn.LayerNorm(in_dim[0])\n",
    "\n",
    "        self.block2 = nn.ModuleList([\n",
    "            FETBlock(in_dim[1], num_heads[1], sr_ratio[1], token_mlp)\n",
    "        for _ in range(layers[1])])\n",
    "        self.norm2 = nn.LayerNorm(in_dim[1])\n",
    "\n",
    "        self.block3 = nn.ModuleList([\n",
    "            FETBlock(in_dim[2], num_heads[2], sr_ratio[2], token_mlp)\n",
    "        for _ in range(layers[2])])\n",
    "        self.norm3 = nn.LayerNorm(in_dim[2])\n",
    "\n",
    "        self.block4 = nn.ModuleList([\n",
    "            FETBlock(in_dim[3], num_heads[3], sr_ratio[3], token_mlp, bottleneck=True)\n",
    "        for _ in range(layers[3])])\n",
    "        self.norm4 = nn.LayerNorm(in_dim[3])\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        # stage 1\n",
    "        x, H, W = self.patch_embed1(x)\n",
    "        for blk in self.block1:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 2\n",
    "        x, H, W = self.patch_embed2(x)\n",
    "        for blk in self.block2:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm2(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 3\n",
    "        x, H, W = self.patch_embed3(x)\n",
    "        for blk in self.block3:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm3(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 4\n",
    "        x, H, W = self.patch_embed4(x)\n",
    "        for blk in self.block4:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm4(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d6684e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of stages from encoder: 4\n",
      "The size of output from the 0 stage: torch.Size([1, 64, 56, 56])\n",
      "The size of output from the 1 stage: torch.Size([1, 128, 28, 28])\n",
      "The size of output from the 2 stage: torch.Size([1, 320, 14, 14])\n",
      "The size of output from the 3 stage: torch.Size([1, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "in_dim, layers = [[64, 128, 320, 512], [2, 2, 2, 2]]\n",
    "num_heads, sr_ratio = [[2, 4, 10, 16], [1, 1, 1, 1]]\n",
    "token_mlp_mode=\"mix_skip\"\n",
    "\n",
    "encoder = Encoder(224, in_dim, num_heads, sr_ratio, layers, token_mlp_mode).cuda()\n",
    "output_enc = encoder(inputs.cuda())\n",
    "print(\"The number of stages from encoder: {}\".format(len(output_enc)))\n",
    "for i in range(len(output_enc)):\n",
    "    print(\"The size of output from the {} stage: {}\".format(i, output_enc[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09644ff",
   "metadata": {},
   "source": [
    "# Skip Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2392a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_1D(nn.Module):\n",
    "    def __init__(self, in_channels, se_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "             nn.AdaptiveAvgPool1d(1),\n",
    "             nn.Conv1d(in_channels, se_channels, kernel_size=1),\n",
    "             nn.GELU(),\n",
    "             nn.Conv1d(se_channels, in_channels, kernel_size=1),\n",
    "             nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.fc(x)\n",
    "        return x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8a3634d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryGenerator(nn.Module):\n",
    "    def __init__(self, dim, dim_out):\n",
    "        super().__init__()\n",
    "        self.se = SE_1D(dim, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, _ = x.shape\n",
    "        x = self.se(x)\n",
    "        \n",
    "        q_lvl1 = x[..., :3136].reshape(B, -1, C)\n",
    "        q_lvl2 = x[..., 3136:4704].reshape(B, -1, C*2)\n",
    "        q_lvl3 = x[..., 4704:5684].reshape(B, -1, C*5)\n",
    "        q_lvl4 = x[..., 5684:6076].reshape(B, -1, C*8)\n",
    "\n",
    "        return [q_lvl1, q_lvl2, q_lvl3, q_lvl4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2cea84e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fext = QueryGenerator(dim=64, dim_out=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f8a16274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136, 64])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fext(torch.rand(1, 64, 6076))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "99482dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BridgeLayer(nn.Module):\n",
    "    def __init__(self, dims, num_heads=[2, 4, 10, 16], sr_ratio=[1, 1, 1, 1]):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.norm1 = nn.LayerNorm(dims)\n",
    "        self.attn = EfficientAttention(dims, dims, dims)\n",
    "        self.norm2 = nn.LayerNorm(dims)\n",
    "        \n",
    "        \n",
    "        # Global Query Generator\n",
    "        self.queries = QueryGenerator(dim=dims, dim_out=dims)\n",
    "        \n",
    "        \n",
    "        # FET Block\n",
    "        self.wave_lvl_1 = FET(dims, num_heads[0], sr_ratio[0], bridge=True)\n",
    "        self.wave_lvl_2 = FET(dims*2, num_heads[1], sr_ratio[1], bridge=True)\n",
    "        self.wave_lvl_3 = FET(dims*5, num_heads[2], sr_ratio[2], bridge=True)\n",
    "        self.wave_lvl_4 = Attention(dims*8, num_heads[3], bridge=True)\n",
    "        \n",
    "        # MixFFN\n",
    "        self.mixffn1 = MixFFN_skip(dims,dims*4)\n",
    "        self.mixffn2 = MixFFN_skip(dims*2,dims*8)\n",
    "        self.mixffn3 = MixFFN_skip(dims*5,dims*20)\n",
    "        self.mixffn4 = MixFFN_skip(dims*8,dims*32)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        B = inputs[0].shape[0]\n",
    "        C = 64\n",
    "        if (type(inputs) == list):\n",
    "            c1, c2, c3, c4 = inputs\n",
    "            B, C, _, _= c1.shape\n",
    "            c1f = c1.permute(0, 2, 3, 1).reshape(B, -1, C)  # 3136*64\n",
    "            c2f = c2.permute(0, 2, 3, 1).reshape(B, -1, C)  # 1568*64\n",
    "            c3f = c3.permute(0, 2, 3, 1).reshape(B, -1, C)  # 980*64\n",
    "            c4f = c4.permute(0, 2, 3, 1).reshape(B, -1, C)  # 392*64\n",
    "            \n",
    "            inputs = torch.cat([c1f, c2f, c3f, c4f], -2)\n",
    "        else:\n",
    "            B,_,C = inputs.shape \n",
    "        \n",
    "        inputs = self.norm1(inputs)\n",
    "        inputs = Rearrange('b (h w) c -> b c h w', h=124, w=49)(inputs)\n",
    "        tx1 = inputs + self.attn(inputs)\n",
    "        tx1 = Rearrange('b c h w -> b (h w) c')(tx1)\n",
    "        tx = self.norm2(tx1)\n",
    "\n",
    "\n",
    "        lvl1 = tx[:,:3136,:].reshape(B, -1, C) \n",
    "        lvl2 = tx[:,3136:4704,:].reshape(B, -1, C*2)\n",
    "        lvl3 = tx[:,4704:5684,:].reshape(B, -1, C*5)\n",
    "        lvl4 = tx[:,5684:6076,:].reshape(B, -1, C*8)\n",
    "        \n",
    "        q_lvl1, q_lvl2, q_lvl3, q_lvl4 = self.queries(tx1.permute(0, 2, 1))\n",
    "\n",
    "        wave_lvl1_att = self.wave_lvl_1(lvl1, 56, 56, q_lvl1)\n",
    "        wave_lvl2_att = self.wave_lvl_2(lvl2, 28, 28, q_lvl2)\n",
    "        wave_lvl3_att = self.wave_lvl_3(lvl3, 14, 14, q_lvl3)\n",
    "        wave_lvl4_att = self.wave_lvl_4(lvl4, 7, 7, q_lvl4)        \n",
    "        \n",
    "        m1f = self.mixffn1(wave_lvl1_att, 56, 56).reshape(B, -1, C)\n",
    "        m2f = self.mixffn2(wave_lvl2_att, 28, 28).reshape(B, -1, C)\n",
    "        m3f = self.mixffn3(wave_lvl3_att, 14, 14).reshape(B, -1, C)\n",
    "        m4f = self.mixffn4(wave_lvl4_att, 7, 7).reshape(B, -1, C)\n",
    "\n",
    "        t1 = torch.cat([m1f, m2f, m3f, m4f], -2)\n",
    "        \n",
    "        tx2 = tx1 + t1\n",
    "\n",
    "        return tx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5d6a41a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_test = BridgeLayer(dims=64, num_heads=[2, 4, 10, 16], sr_ratio=[1, 1, 1, 1]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7206198b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6076, 64])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_test(output_enc).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fdc88668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BridegeBlock(nn.Module):\n",
    "    def __init__(self, dims, num_heads=[2, 4, 10, 16], sr_ratio=[1, 1, 1, 1], bridge_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bridge = nn.ModuleList([ \n",
    "            BridgeLayer(dims, num_heads=num_heads, sr_ratio=sr_ratio)\n",
    "        for _ in range(bridge_layers)])\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        for blk in self.bridge:\n",
    "            x = blk(x)\n",
    "\n",
    "        B,_,C = x.shape\n",
    "        outs = []\n",
    "\n",
    "        sk1 = x[:,:3136,:].reshape(B, 56, 56, C).permute(0,3,1,2) \n",
    "        sk2 = x[:,3136:4704,:].reshape(B, 28, 28, C*2).permute(0,3,1,2) \n",
    "        sk3 = x[:,4704:5684,:].reshape(B, 14, 14, C*5).permute(0,3,1,2) \n",
    "        sk4 = x[:,5684:6076,:].reshape(B, 7, 7, C*8).permute(0,3,1,2) \n",
    "\n",
    "        outs.append(sk1)\n",
    "        outs.append(sk2)\n",
    "        outs.append(sk3)\n",
    "        outs.append(sk4)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0941216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_block = BridegeBlock(dims=64, num_heads=[2, 4, 10, 16], sr_ratio=[1, 1, 1, 1]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4a366922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 64, 56, 56]),\n",
       " torch.Size([1, 128, 28, 28]),\n",
       " torch.Size([1, 320, 14, 14]),\n",
       " torch.Size([1, 512, 7, 7]))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_block(output_enc)[0].shape, b_block(output_enc)[1].shape, b_block(output_enc)[2].shape, b_block(output_enc)[3].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f227b4f0",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "300aa56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExpand(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.expand = nn.Linear(dim, 2*dim, bias=False) if dim_scale==2 else nn.Identity()\n",
    "        self.norm = norm_layer(dim // dim_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        # print(\"x_shape-----\",x.shape)\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "        \n",
    "        B, L, C = x.shape\n",
    "        # print(x.shape)\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=2, p2=2, c=C//4)\n",
    "        x = x.view(B,-1,C//4)\n",
    "        x= self.norm(x.clone())\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6aed7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalPatchExpand_X4(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=4, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(dim, 16*dim, bias=False)\n",
    "        self.output_dim = dim \n",
    "        self.norm = norm_layer(self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale, c=C//(self.dim_scale**2))\n",
    "        x = x.view(B,-1,self.output_dim)\n",
    "        x= self.norm(x.clone())\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "37916e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoderLayer(nn.Module):\n",
    "    def __init__(self, input_size, in_out_head_sr, token_mlp_mode, n_class=9,\n",
    "                 norm_layer=nn.LayerNorm, is_last=False):\n",
    "        super().__init__()\n",
    "        dims = in_out_head_sr[0]\n",
    "        out_dim = in_out_head_sr[1]\n",
    "        num_heads = in_out_head_sr[2]\n",
    "        sr_ratio = in_out_head_sr[3]\n",
    "        \n",
    "        if not is_last:\n",
    "            self.concat_linear = nn.Linear(dims*2, out_dim)\n",
    "            self.layer_up = PatchExpand(input_resolution=input_size, dim=out_dim, dim_scale=2, norm_layer=norm_layer)\n",
    "            self.last_layer = None\n",
    "        else:\n",
    "            self.concat_linear = nn.Linear(dims*4, out_dim)\n",
    "            self.layer_up = FinalPatchExpand_X4(input_resolution=input_size, dim=out_dim, dim_scale=4, norm_layer=norm_layer)\n",
    "            self.last_layer = nn.Conv2d(out_dim, n_class,1)\n",
    "\n",
    "        self.layer_former_1 = FETBlock(out_dim, num_heads, sr_ratio, token_mlp_mode)\n",
    "        self.layer_former_2 = FETBlock(out_dim, num_heads, sr_ratio, token_mlp_mode)\n",
    "       \n",
    "\n",
    "        def init_weights(self): \n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "        init_weights(self)\n",
    "      \n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:# skip connection exist\n",
    "            print(\"x1 shape:\", x1.shape)\n",
    "            print(\"x2 shape:\", x2.shape)\n",
    "            b, h, w, c = x2.shape\n",
    "            x2 = x2.view(b, -1, c)\n",
    "            print(\"------\",x1.shape, x2.shape)\n",
    "            cat_x = torch.cat([x1, x2], dim=-1)\n",
    "            print(\"-----catx shape:\", cat_x.shape)\n",
    "            cat_linear_x = self.concat_linear(cat_x)\n",
    "            print('cat_linear_x shape:', cat_linear_x.shape)\n",
    "            tran_layer_1 = self.layer_former_1(cat_linear_x, h, w)\n",
    "            tran_layer_2 = self.layer_former_2(tran_layer_1, h, w)\n",
    "            \n",
    "            if self.last_layer:\n",
    "                out = self.last_layer(self.layer_up(tran_layer_2).view(b, 4*h, 4*w, -1).permute(0,3,1,2)) \n",
    "            else:\n",
    "                out = self.layer_up(tran_layer_2)\n",
    "        else:\n",
    "            print(\"x1 shape\",x1.shape)\n",
    "            out = self.layer_up(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b1585",
   "metadata": {},
   "source": [
    "# The EfficientMISSFormer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "abab6df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes=9, num_heads_enc=[2, 4, 10, 16], sr_ratio=[1, 1, 1, 1], num_heads_dec= [16, 8, 4, 1],\n",
    "                 bridge_layers=2, token_mlp_mode=\"mix_skip\"):\n",
    "        super().__init__()\n",
    "    \n",
    "        # Encoder\n",
    "        dims, layers = [[64, 128, 320, 512], [2, 2, 2, 2]]\n",
    "        self.backbone = Encoder(image_size=224, in_dim=dims, num_heads=num_heads_enc, sr_ratio=sr_ratio, layers=layers,\n",
    "                                token_mlp=token_mlp_mode)\n",
    "        self.skip_connection = BridegeBlock(dims=dims[0], num_heads=num_heads_enc, sr_ratio=[1, 1, 1, 1],\n",
    "                                            bridge_layers=bridge_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        d_base_feat_size = 7 #16 for 512 input size, and 7 for 224\n",
    "        in_out_head_sr = [[32, 64, num_heads_dec[0], sr_ratio[-1]], [144, 128, num_heads_dec[1], sr_ratio[-2]],\n",
    "                          [288, 320, num_heads_dec[2], sr_ratio[-3]], [512, 512, num_heads_dec[3], sr_ratio[-4]]]\n",
    "\n",
    "        self.decoder_3 = MyDecoderLayer((d_base_feat_size, d_base_feat_size), in_out_head_sr[3], \n",
    "                                        token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_2 = MyDecoderLayer((d_base_feat_size*2, d_base_feat_size*2), in_out_head_sr[2],\n",
    "                                        token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_1 = MyDecoderLayer((d_base_feat_size*4, d_base_feat_size*4), in_out_head_sr[1], \n",
    "                                        token_mlp_mode, n_class=num_classes) \n",
    "        self.decoder_0 = MyDecoderLayer((d_base_feat_size*8, d_base_feat_size*8), in_out_head_sr[0],\n",
    "                                        token_mlp_mode, n_class=num_classes, is_last=True)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #---------------Encoder-------------------------\n",
    "        if x.size()[1] == 1:\n",
    "            x = x.repeat(1,3,1,1)\n",
    "\n",
    "        output_enc = self.backbone(x)\n",
    "        output_enc = self.skip_connection(output_enc)\n",
    "        \n",
    "        b,c,_,_ = output_enc[3].shape\n",
    "        \n",
    "        #---------------Decoder-------------------------     \n",
    "        tmp_3 = self.decoder_3(output_enc[3].permute(0,2,3,1).view(b,-1,c))\n",
    "        \n",
    "        print('\\n', \"stage2-----\")   \n",
    "        tmp_2 = self.decoder_2(tmp_3, output_enc[2].permute(0,2,3,1))\n",
    "        \n",
    "        print('\\n', \"stage1-----\")   \n",
    "        tmp_1 = self.decoder_1(tmp_2, output_enc[1].permute(0,2,3,1))\n",
    "        \n",
    "        print('\\n', \"stage0-----\")  \n",
    "        tmp_0 = self.decoder_0(tmp_1, output_enc[0].permute(0,2,3,1))\n",
    "        \n",
    "        print('\\n', '---------------Decoder------------------')\n",
    "        print(output_enc[3].shape)\n",
    "        print(tmp_3.shape)\n",
    "        print(tmp_2.shape)\n",
    "        print(tmp_1.shape)\n",
    "        print(tmp_0.shape)\n",
    "\n",
    "        return tmp_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5bdbca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = WaveFormer().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a606cb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 shape torch.Size([1, 49, 512])\n",
      "\n",
      " stage2-----\n",
      "x1 shape: torch.Size([1, 196, 256])\n",
      "x2 shape: torch.Size([1, 14, 14, 320])\n",
      "------ torch.Size([1, 196, 256]) torch.Size([1, 196, 320])\n",
      "-----catx shape: torch.Size([1, 196, 576])\n",
      "cat_linear_x shape: torch.Size([1, 196, 320])\n",
      "\n",
      " stage1-----\n",
      "x1 shape: torch.Size([1, 784, 160])\n",
      "x2 shape: torch.Size([1, 28, 28, 128])\n",
      "------ torch.Size([1, 784, 160]) torch.Size([1, 784, 128])\n",
      "-----catx shape: torch.Size([1, 784, 288])\n",
      "cat_linear_x shape: torch.Size([1, 784, 128])\n",
      "\n",
      " stage0-----\n",
      "x1 shape: torch.Size([1, 3136, 64])\n",
      "x2 shape: torch.Size([1, 56, 56, 64])\n",
      "------ torch.Size([1, 3136, 64]) torch.Size([1, 3136, 64])\n",
      "-----catx shape: torch.Size([1, 3136, 128])\n",
      "cat_linear_x shape: torch.Size([1, 3136, 64])\n",
      "\n",
      " ---------------Decoder------------------\n",
      "torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 196, 256])\n",
      "torch.Size([1, 784, 160])\n",
      "torch.Size([1, 3136, 64])\n",
      "torch.Size([1, 9, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 224, 224])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(torch.rand(1, 3, 224, 224).cuda()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fec8dda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 shape torch.Size([1, 49, 512])\n",
      "\n",
      " stage2-----\n",
      "x1 shape: torch.Size([1, 196, 256])\n",
      "x2 shape: torch.Size([1, 14, 14, 320])\n",
      "------ torch.Size([1, 196, 256]) torch.Size([1, 196, 320])\n",
      "-----catx shape: torch.Size([1, 196, 576])\n",
      "cat_linear_x shape: torch.Size([1, 196, 320])\n",
      "\n",
      " stage1-----\n",
      "x1 shape: torch.Size([1, 784, 160])\n",
      "x2 shape: torch.Size([1, 28, 28, 128])\n",
      "------ torch.Size([1, 784, 160]) torch.Size([1, 784, 128])\n",
      "-----catx shape: torch.Size([1, 784, 288])\n",
      "cat_linear_x shape: torch.Size([1, 784, 128])\n",
      "\n",
      " stage0-----\n",
      "x1 shape: torch.Size([1, 3136, 64])\n",
      "x2 shape: torch.Size([1, 56, 56, 64])\n",
      "------ torch.Size([1, 3136, 64]) torch.Size([1, 3136, 64])\n",
      "-----catx shape: torch.Size([1, 3136, 128])\n",
      "cat_linear_x shape: torch.Size([1, 3136, 64])\n",
      "\n",
      " ---------------Decoder------------------\n",
      "torch.Size([1, 512, 7, 7])\n",
      "torch.Size([1, 196, 256])\n",
      "torch.Size([1, 784, 160])\n",
      "torch.Size([1, 3136, 64])\n",
      "torch.Size([1, 9, 224, 224])\n",
      "| module                                    | #parameters or shape   | #flops     |\n",
      "|:------------------------------------------|:-----------------------|:-----------|\n",
      "| model                                     | 45.033M                | 6.167G     |\n",
      "|  backbone                                 |  13.919M               |  3.194G    |\n",
      "|   backbone.patch_embed1                   |   9.6K                 |   30.507M  |\n",
      "|    backbone.patch_embed1.proj             |    9.472K              |    29.503M |\n",
      "|    backbone.patch_embed1.norm             |    0.128K              |    1.004M  |\n",
      "|   backbone.patch_embed2                   |   74.112K              |   58.305M  |\n",
      "|    backbone.patch_embed2.proj             |    73.856K             |    57.803M |\n",
      "|    backbone.patch_embed2.norm             |    0.256K              |    0.502M  |\n",
      "|   backbone.patch_embed3                   |   0.37M                |   72.567M  |\n",
      "|    backbone.patch_embed3.proj             |    0.369M              |    72.253M |\n",
      "|    backbone.patch_embed3.norm             |    0.64K               |    0.314M  |\n",
      "|   backbone.patch_embed4                   |   1.476M               |   72.379M  |\n",
      "|    backbone.patch_embed4.proj             |    1.475M              |    72.253M |\n",
      "|    backbone.patch_embed4.norm             |    1.024K              |    0.125M  |\n",
      "|   backbone.block1                         |   0.195M               |   1.154G   |\n",
      "|    backbone.block1.0                      |    97.616K             |    0.577G  |\n",
      "|    backbone.block1.1                      |    97.616K             |    0.577G  |\n",
      "|   backbone.norm1                          |   0.128K               |   1.004M   |\n",
      "|    backbone.norm1.weight                  |    (64,)               |            |\n",
      "|    backbone.norm1.bias                    |    (64,)               |            |\n",
      "|   backbone.block2                         |   0.759M               |   0.589G   |\n",
      "|    backbone.block2.0                      |    0.38M               |    0.294G  |\n",
      "|    backbone.block2.1                      |    0.38M               |    0.294G  |\n",
      "|   backbone.norm2                          |   0.256K               |   0.502M   |\n",
      "|    backbone.norm2.weight                  |    (128,)              |            |\n",
      "|    backbone.norm2.bias                    |    (128,)              |            |\n",
      "|   backbone.block3                         |   4.663M               |   0.796G   |\n",
      "|    backbone.block3.0                      |    2.331M              |    0.398G  |\n",
      "|    backbone.block3.1                      |    2.331M              |    0.398G  |\n",
      "|   backbone.norm3                          |   0.64K                |   0.314M   |\n",
      "|    backbone.norm3.weight                  |    (320,)              |            |\n",
      "|    backbone.norm3.bias                    |    (320,)              |            |\n",
      "|   backbone.block4                         |   6.37M                |   0.419G   |\n",
      "|    backbone.block4.0                      |    3.185M              |    0.21G   |\n",
      "|    backbone.block4.1                      |    3.185M              |    0.21G   |\n",
      "|   backbone.norm4                          |   1.024K               |   0.125M   |\n",
      "|    backbone.norm4.weight                  |    (512,)              |            |\n",
      "|    backbone.norm4.bias                    |    (512,)              |            |\n",
      "|  skip_connection.bridge                   |  12.029M               |            |\n",
      "|   skip_connection.bridge.0                |   6.015M               |            |\n",
      "|    skip_connection.bridge.0.norm1         |    0.128K              |            |\n",
      "|    skip_connection.bridge.0.attn          |    16.64K              |            |\n",
      "|    skip_connection.bridge.0.norm2         |    0.128K              |            |\n",
      "|    skip_connection.bridge.0.queries.se.fc |    8.32K               |            |\n",
      "|    skip_connection.bridge.0.wave_lvl_1    |    60.176K             |            |\n",
      "|    skip_connection.bridge.0.wave_lvl_2    |    0.239M              |            |\n",
      "|    skip_connection.bridge.0.wave_lvl_3    |    1.489M              |            |\n",
      "|    skip_connection.bridge.0.wave_lvl_4    |    1.051M              |            |\n",
      "|    skip_connection.bridge.0.mixffn1       |    37.184K             |            |\n",
      "|    skip_connection.bridge.0.mixffn2       |    0.14M               |            |\n",
      "|    skip_connection.bridge.0.mixffn3       |    0.841M              |            |\n",
      "|    skip_connection.bridge.0.mixffn4       |    2.132M              |            |\n",
      "|   skip_connection.bridge.1                |   6.015M               |            |\n",
      "|    skip_connection.bridge.1.norm1         |    0.128K              |            |\n",
      "|    skip_connection.bridge.1.attn          |    16.64K              |            |\n",
      "|    skip_connection.bridge.1.norm2         |    0.128K              |            |\n",
      "|    skip_connection.bridge.1.queries.se.fc |    8.32K               |            |\n",
      "|    skip_connection.bridge.1.wave_lvl_1    |    60.176K             |            |\n",
      "|    skip_connection.bridge.1.wave_lvl_2    |    0.239M              |            |\n",
      "|    skip_connection.bridge.1.wave_lvl_3    |    1.489M              |            |\n",
      "|    skip_connection.bridge.1.wave_lvl_4    |    1.051M              |            |\n",
      "|    skip_connection.bridge.1.mixffn1       |    37.184K             |            |\n",
      "|    skip_connection.bridge.1.mixffn2       |    0.14M               |            |\n",
      "|    skip_connection.bridge.1.mixffn3       |    0.841M              |            |\n",
      "|    skip_connection.bridge.1.mixffn4       |    2.132M              |            |\n",
      "|  decoder_3                                |  12.933M               |  25.941M   |\n",
      "|   decoder_3.concat_linear                 |   0.525M               |            |\n",
      "|    decoder_3.concat_linear.weight         |    (512, 1024)         |            |\n",
      "|    decoder_3.concat_linear.bias           |    (512,)              |            |\n",
      "|   decoder_3.layer_up                      |   0.525M               |   25.941M  |\n",
      "|    decoder_3.layer_up.expand              |    0.524M              |    25.69M  |\n",
      "|    decoder_3.layer_up.norm                |    0.512K              |    0.251M  |\n",
      "|   decoder_3.layer_former_1                |   5.942M               |            |\n",
      "|    decoder_3.layer_former_1.norm1         |    1.024K              |            |\n",
      "|    decoder_3.layer_former_1.attn          |    3.807M              |            |\n",
      "|    decoder_3.layer_former_1.norm2         |    1.024K              |            |\n",
      "|    decoder_3.layer_former_1.mlp           |    2.132M              |            |\n",
      "|   decoder_3.layer_former_2                |   5.942M               |            |\n",
      "|    decoder_3.layer_former_2.norm1         |    1.024K              |            |\n",
      "|    decoder_3.layer_former_2.attn          |    3.807M              |            |\n",
      "|    decoder_3.layer_former_2.norm2         |    1.024K              |            |\n",
      "|    decoder_3.layer_former_2.mlp           |    2.132M              |            |\n",
      "|  decoder_2                                |  5.052M                |  0.873G    |\n",
      "|   decoder_2.concat_linear                 |   0.185M               |   36.127M  |\n",
      "|    decoder_2.concat_linear.weight         |    (320, 576)          |            |\n",
      "|    decoder_2.concat_linear.bias           |    (320,)              |            |\n",
      "|   decoder_2.layer_up                      |   0.205M               |   40.768M  |\n",
      "|    decoder_2.layer_up.expand              |    0.205M              |    40.141M |\n",
      "|    decoder_2.layer_up.norm                |    0.32K               |    0.627M  |\n",
      "|   decoder_2.layer_former_1                |   2.331M               |   0.398G   |\n",
      "|    decoder_2.layer_former_1.norm1         |    0.64K               |    0.314M  |\n",
      "|    decoder_2.layer_former_1.attn          |    1.489M              |    0.153G  |\n",
      "|    decoder_2.layer_former_1.norm2         |    0.64K               |    0.314M  |\n",
      "|    decoder_2.layer_former_1.mlp           |    0.841M              |    0.244G  |\n",
      "|   decoder_2.layer_former_2                |   2.331M               |   0.398G   |\n",
      "|    decoder_2.layer_former_2.norm1         |    0.64K               |    0.314M  |\n",
      "|    decoder_2.layer_former_2.attn          |    1.489M              |    0.153G  |\n",
      "|    decoder_2.layer_former_2.norm2         |    0.64K               |    0.314M  |\n",
      "|    decoder_2.layer_former_2.mlp           |    0.841M              |    0.244G  |\n",
      "|  decoder_1                                |  0.829M                |  0.644G    |\n",
      "|   decoder_1.concat_linear                 |   36.992K              |   28.901M  |\n",
      "|    decoder_1.concat_linear.weight         |    (128, 288)          |            |\n",
      "|    decoder_1.concat_linear.bias           |    (128,)              |            |\n",
      "|   decoder_1.layer_up                      |   32.896K              |   26.694M  |\n",
      "|    decoder_1.layer_up.expand              |    32.768K             |    25.69M  |\n",
      "|    decoder_1.layer_up.norm                |    0.128K              |    1.004M  |\n",
      "|   decoder_1.layer_former_1                |   0.38M                |   0.294G   |\n",
      "|    decoder_1.layer_former_1.norm1         |    0.256K              |    0.502M  |\n",
      "|    decoder_1.layer_former_1.attn          |    0.239M              |    0.134G  |\n",
      "|    decoder_1.layer_former_1.norm2         |    0.256K              |    0.502M  |\n",
      "|    decoder_1.layer_former_1.mlp           |    0.14M               |    0.16G   |\n",
      "|   decoder_1.layer_former_2                |   0.38M                |   0.294G   |\n",
      "|    decoder_1.layer_former_2.norm1         |    0.256K              |    0.502M  |\n",
      "|    decoder_1.layer_former_2.attn          |    0.239M              |    0.134G  |\n",
      "|    decoder_1.layer_former_2.norm2         |    0.256K              |    0.502M  |\n",
      "|    decoder_1.layer_former_2.mlp           |    0.14M               |    0.16G   |\n",
      "|  decoder_0                                |  0.27M                 |  1.43G     |\n",
      "|   decoder_0.concat_linear                 |   8.256K               |   25.69M   |\n",
      "|    decoder_0.concat_linear.weight         |    (64, 128)           |            |\n",
      "|    decoder_0.concat_linear.bias           |    (64,)               |            |\n",
      "|   decoder_0.layer_up                      |   65.664K              |   0.222G   |\n",
      "|    decoder_0.layer_up.expand              |    65.536K             |    0.206G  |\n",
      "|    decoder_0.layer_up.norm                |    0.128K              |    16.056M |\n",
      "|   decoder_0.last_layer                    |   0.585K               |   28.901M  |\n",
      "|    decoder_0.last_layer.weight            |    (9, 64, 1, 1)       |            |\n",
      "|    decoder_0.last_layer.bias              |    (9,)                |            |\n",
      "|   decoder_0.layer_former_1                |   97.616K              |   0.577G   |\n",
      "|    decoder_0.layer_former_1.norm1         |    0.128K              |    1.004M  |\n",
      "|    decoder_0.layer_former_1.attn          |    60.176K             |    0.409G  |\n",
      "|    decoder_0.layer_former_1.norm2         |    0.128K              |    1.004M  |\n",
      "|    decoder_0.layer_former_1.mlp           |    37.184K             |    0.165G  |\n",
      "|   decoder_0.layer_former_2                |   97.616K              |   0.577G   |\n",
      "|    decoder_0.layer_former_2.norm1         |    0.128K              |    1.004M  |\n",
      "|    decoder_0.layer_former_2.attn          |    60.176K             |    0.409G  |\n",
      "|    decoder_0.layer_former_2.norm2         |    0.128K              |    1.004M  |\n",
      "|    decoder_0.layer_former_2.mlp           |    37.184K             |    0.165G  |\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import flop_count_table\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "\n",
    "flops = FlopCountAnalysis(test, torch.rand(1, 3, 224, 224).cuda())\n",
    "print(flop_count_table(flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "48d99df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| module                                    | #parameters or shape   | #flops     |\n",
      "|:------------------------------------------|:-----------------------|:-----------|\n",
      "| model                                     | 45.033M                | 6.167G     |\n",
      "|  backbone                                 |  13.919M               |  3.194G    |\n",
      "|   backbone.patch_embed1                   |   9.6K                 |   30.507M  |\n",
      "|    backbone.patch_embed1.proj             |    9.472K              |    29.503M |\n",
      "|    backbone.patch_embed1.norm             |    0.128K              |    1.004M  |\n",
      "|   backbone.patch_embed2                   |   74.112K              |   58.305M  |\n",
      "|    backbone.patch_embed2.proj             |    73.856K             |    57.803M |\n",
      "|    backbone.patch_embed2.norm             |    0.256K              |    0.502M  |\n",
      "|   backbone.patch_embed3                   |   0.37M                |   72.567M  |\n",
      "|    backbone.patch_embed3.proj             |    0.369M              |    72.253M |\n",
      "|    backbone.patch_embed3.norm             |    0.64K               |    0.314M  |\n",
      "|   backbone.patch_embed4                   |   1.476M               |   72.379M  |\n",
      "|    backbone.patch_embed4.proj             |    1.475M              |    72.253M |\n",
      "|    backbone.patch_embed4.norm             |    1.024K              |    0.125M  |\n",
      "|   backbone.block1                         |   0.195M               |   1.154G   |\n",
      "|    backbone.block1.0                      |    97.616K             |    0.577G  |\n",
      "|    backbone.block1.1                      |    97.616K             |    0.577G  |\n",
      "|   backbone.norm1                          |   0.128K               |   1.004M   |\n",
      "|    backbone.norm1.weight                  |    (64,)               |            |\n",
      "|    backbone.norm1.bias                    |    (64,)               |            |\n",
      "|   backbone.block2                         |   0.759M               |   0.589G   |\n",
      "|    backbone.block2.0                      |    0.38M               |    0.294G  |\n",
      "|    backbone.block2.1                      |    0.38M               |    0.294G  |\n",
      "|   backbone.norm2                          |   0.256K               |   0.502M   |\n",
      "|    backbone.norm2.weight                  |    (128,)              |            |\n",
      "|    backbone.norm2.bias                    |    (128,)              |            |\n",
      "|   backbone.block3                         |   4.663M               |   0.796G   |\n",
      "|    backbone.block3.0                      |    2.331M              |    0.398G  |\n",
      "|    backbone.block3.1                      |    2.331M              |    0.398G  |\n",
      "|   backbone.norm3                          |   0.64K                |   0.314M   |\n",
      "|    backbone.norm3.weight                  |    (320,)              |            |\n",
      "|    backbone.norm3.bias                    |    (320,)              |            |\n",
      "|   backbone.block4                         |   6.37M                |   0.419G   |\n",
      "|    backbone.block4.0                      |    3.185M              |    0.21G   |\n",
      "|    backbone.block4.1                      |    3.185M              |    0.21G   |\n",
      "|   backbone.norm4                          |   1.024K               |   0.125M   |\n",
      "|    backbone.norm4.weight                  |    (512,)              |            |\n",
      "|    backbone.norm4.bias                    |    (512,)              |            |\n",
      "|  skip_connection.bridge                   |  12.029M               |            |\n",
      "|   skip_connection.bridge.0                |   6.015M               |            |\n",
      "|    skip_connection.bridge.0.norm1         |    0.128K              |            |\n",
      "|    skip_connection.bridge.0.attn          |    16.64K              |            |\n",
      "|    skip_connection.bridge.0.norm2         |    0.128K              |            |\n",
      "|    skip_connection.bridge.0.queries.se.fc |    8.32K               |            |\n",
      "|    skip_connection.bridge.0.wave_lvl_1    |    60.176K             |            |\n",
      "|    skip_connection.bridge.0.wave_lvl_2    |    0.239M              |            |\n",
      "|    skip_connection.bridge.0.wave_lvl_3    |    1.489M              |            |\n",
      "|    skip_connection.bridge.0.wave_lvl_4    |    1.051M              |            |\n",
      "|    skip_connection.bridge.0.mixffn1       |    37.184K             |            |\n",
      "|    skip_connection.bridge.0.mixffn2       |    0.14M               |            |\n",
      "|    skip_connection.bridge.0.mixffn3       |    0.841M              |            |\n",
      "|    skip_connection.bridge.0.mixffn4       |    2.132M              |            |\n",
      "|   skip_connection.bridge.1                |   6.015M               |            |\n",
      "|    skip_connection.bridge.1.norm1         |    0.128K              |            |\n",
      "|    skip_connection.bridge.1.attn          |    16.64K              |            |\n",
      "|    skip_connection.bridge.1.norm2         |    0.128K              |            |\n",
      "|    skip_connection.bridge.1.queries.se.fc |    8.32K               |            |\n",
      "|    skip_connection.bridge.1.wave_lvl_1    |    60.176K             |            |\n",
      "|    skip_connection.bridge.1.wave_lvl_2    |    0.239M              |            |\n",
      "|    skip_connection.bridge.1.wave_lvl_3    |    1.489M              |            |\n",
      "|    skip_connection.bridge.1.wave_lvl_4    |    1.051M              |            |\n",
      "|    skip_connection.bridge.1.mixffn1       |    37.184K             |            |\n",
      "|    skip_connection.bridge.1.mixffn2       |    0.14M               |            |\n",
      "|    skip_connection.bridge.1.mixffn3       |    0.841M              |            |\n",
      "|    skip_connection.bridge.1.mixffn4       |    2.132M              |            |\n",
      "|  decoder_3                                |  12.933M               |  25.941M   |\n",
      "|   decoder_3.concat_linear                 |   0.525M               |            |\n",
      "|    decoder_3.concat_linear.weight         |    (512, 1024)         |            |\n",
      "|    decoder_3.concat_linear.bias           |    (512,)              |            |\n",
      "|   decoder_3.layer_up                      |   0.525M               |   25.941M  |\n",
      "|    decoder_3.layer_up.expand              |    0.524M              |    25.69M  |\n",
      "|    decoder_3.layer_up.norm                |    0.512K              |    0.251M  |\n",
      "|   decoder_3.layer_former_1                |   5.942M               |            |\n",
      "|    decoder_3.layer_former_1.norm1         |    1.024K              |            |\n",
      "|    decoder_3.layer_former_1.attn          |    3.807M              |            |\n",
      "|    decoder_3.layer_former_1.norm2         |    1.024K              |            |\n",
      "|    decoder_3.layer_former_1.mlp           |    2.132M              |            |\n",
      "|   decoder_3.layer_former_2                |   5.942M               |            |\n",
      "|    decoder_3.layer_former_2.norm1         |    1.024K              |            |\n",
      "|    decoder_3.layer_former_2.attn          |    3.807M              |            |\n",
      "|    decoder_3.layer_former_2.norm2         |    1.024K              |            |\n",
      "|    decoder_3.layer_former_2.mlp           |    2.132M              |            |\n",
      "|  decoder_2                                |  5.052M                |  0.873G    |\n",
      "|   decoder_2.concat_linear                 |   0.185M               |   36.127M  |\n",
      "|    decoder_2.concat_linear.weight         |    (320, 576)          |            |\n",
      "|    decoder_2.concat_linear.bias           |    (320,)              |            |\n",
      "|   decoder_2.layer_up                      |   0.205M               |   40.768M  |\n",
      "|    decoder_2.layer_up.expand              |    0.205M              |    40.141M |\n",
      "|    decoder_2.layer_up.norm                |    0.32K               |    0.627M  |\n",
      "|   decoder_2.layer_former_1                |   2.331M               |   0.398G   |\n",
      "|    decoder_2.layer_former_1.norm1         |    0.64K               |    0.314M  |\n",
      "|    decoder_2.layer_former_1.attn          |    1.489M              |    0.153G  |\n",
      "|    decoder_2.layer_former_1.norm2         |    0.64K               |    0.314M  |\n",
      "|    decoder_2.layer_former_1.mlp           |    0.841M              |    0.244G  |\n",
      "|   decoder_2.layer_former_2                |   2.331M               |   0.398G   |\n",
      "|    decoder_2.layer_former_2.norm1         |    0.64K               |    0.314M  |\n",
      "|    decoder_2.layer_former_2.attn          |    1.489M              |    0.153G  |\n",
      "|    decoder_2.layer_former_2.norm2         |    0.64K               |    0.314M  |\n",
      "|    decoder_2.layer_former_2.mlp           |    0.841M              |    0.244G  |\n",
      "|  decoder_1                                |  0.829M                |  0.644G    |\n",
      "|   decoder_1.concat_linear                 |   36.992K              |   28.901M  |\n",
      "|    decoder_1.concat_linear.weight         |    (128, 288)          |            |\n",
      "|    decoder_1.concat_linear.bias           |    (128,)              |            |\n",
      "|   decoder_1.layer_up                      |   32.896K              |   26.694M  |\n",
      "|    decoder_1.layer_up.expand              |    32.768K             |    25.69M  |\n",
      "|    decoder_1.layer_up.norm                |    0.128K              |    1.004M  |\n",
      "|   decoder_1.layer_former_1                |   0.38M                |   0.294G   |\n",
      "|    decoder_1.layer_former_1.norm1         |    0.256K              |    0.502M  |\n",
      "|    decoder_1.layer_former_1.attn          |    0.239M              |    0.134G  |\n",
      "|    decoder_1.layer_former_1.norm2         |    0.256K              |    0.502M  |\n",
      "|    decoder_1.layer_former_1.mlp           |    0.14M               |    0.16G   |\n",
      "|   decoder_1.layer_former_2                |   0.38M                |   0.294G   |\n",
      "|    decoder_1.layer_former_2.norm1         |    0.256K              |    0.502M  |\n",
      "|    decoder_1.layer_former_2.attn          |    0.239M              |    0.134G  |\n",
      "|    decoder_1.layer_former_2.norm2         |    0.256K              |    0.502M  |\n",
      "|    decoder_1.layer_former_2.mlp           |    0.14M               |    0.16G   |\n",
      "|  decoder_0                                |  0.27M                 |  1.43G     |\n",
      "|   decoder_0.concat_linear                 |   8.256K               |   25.69M   |\n",
      "|    decoder_0.concat_linear.weight         |    (64, 128)           |            |\n",
      "|    decoder_0.concat_linear.bias           |    (64,)               |            |\n",
      "|   decoder_0.layer_up                      |   65.664K              |   0.222G   |\n",
      "|    decoder_0.layer_up.expand              |    65.536K             |    0.206G  |\n",
      "|    decoder_0.layer_up.norm                |    0.128K              |    16.056M |\n",
      "|   decoder_0.last_layer                    |   0.585K               |   28.901M  |\n",
      "|    decoder_0.last_layer.weight            |    (9, 64, 1, 1)       |            |\n",
      "|    decoder_0.last_layer.bias              |    (9,)                |            |\n",
      "|   decoder_0.layer_former_1                |   97.616K              |   0.577G   |\n",
      "|    decoder_0.layer_former_1.norm1         |    0.128K              |    1.004M  |\n",
      "|    decoder_0.layer_former_1.attn          |    60.176K             |    0.409G  |\n",
      "|    decoder_0.layer_former_1.norm2         |    0.128K              |    1.004M  |\n",
      "|    decoder_0.layer_former_1.mlp           |    37.184K             |    0.165G  |\n",
      "|   decoder_0.layer_former_2                |   97.616K              |   0.577G   |\n",
      "|    decoder_0.layer_former_2.norm1         |    0.128K              |    1.004M  |\n",
      "|    decoder_0.layer_former_2.attn          |    60.176K             |    0.409G  |\n",
      "|    decoder_0.layer_former_2.norm2         |    0.128K              |    1.004M  |\n",
      "|    decoder_0.layer_former_2.mlp           |    37.184K             |    0.165G  |\n"
     ]
    }
   ],
   "source": [
    "print(flop_count_table(flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75085a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c33ef0d4398937ecb5268daa690e9c83c694125b3e6c89d9a5ef925d0e1e9cba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
